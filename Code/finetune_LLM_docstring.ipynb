{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:24.685238Z",
     "end_time": "2024-12-27T19:24:27.890495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: peft in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: transformers in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (4.47.1)\r\n",
      "Requirement already satisfied: trl in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: datasets in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: wandb in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.19.1)\r\n",
      "Requirement already satisfied: mlflow in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (2.19.0)\r\n",
      "Requirement already satisfied: pyngrok in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (7.2.2)\r\n",
      "Requirement already satisfied: numpy==1.24.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (24.1)\r\n",
      "Requirement already satisfied: psutil in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (0.27.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from peft) (4.67.1)\r\n",
      "Requirement already satisfied: scipy in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from bitsandbytes) (1.14.1)\r\n",
      "Requirement already satisfied: filelock in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (3.16.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: rich in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from trl) (13.9.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: xxhash in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (3.11.10)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (4.3.6)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (5.29.1)\r\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (2.10.4)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (2.19.2)\r\n",
      "Requirement already satisfied: setproctitle in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (1.3.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (67.7.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: mlflow-skinny==2.19.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (2.19.0)\r\n",
      "Requirement already satisfied: Flask<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.1.4)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (1.14.0)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (7.1.0)\r\n",
      "Requirement already satisfied: graphene<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.4.3)\r\n",
      "Requirement already satisfied: gunicorn<24 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (23.0.0)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.7)\r\n",
      "Requirement already satisfied: matplotlib<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.9.2)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (1.5.2)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (2.0.36)\r\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.39.0)\r\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\r\n",
      "Requirement already satisfied: Mako in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (1.9.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.5)\r\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.54.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.7)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.5.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from rich->trl) (2.18.0)\r\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Configuring the character encoding\n",
    "import locale\n",
    "\n",
    "\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "!pip install -U accelerate peft bitsandbytes transformers trl datasets wandb mlflow pyngrok numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import numpy\n",
    "import random\n",
    "import mlflow\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Features, Value\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from pyngrok import ngrok"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:29.583557Z",
     "end_time": "2024-12-27T19:24:36.290396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is running locally.\n"
     ]
    }
   ],
   "source": [
    "# Check if the notebook runs on Colab to adjust paths\n",
    "on_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if on_colab:\n",
    "    test_file = \"/content/drive/MyDrive/Data/docstring_test_data.txt\"\n",
    "    train_file = \"/content/drive/MyDrive/Data/docstring_training_data.txt\"\n",
    "    base_output_dir = \"/content/drive/MyDrive/Models/\"\n",
    "    print(\"The notebook runs on Google Colab.\")\n",
    "else:\n",
    "    test_file = \"../Data/docstring_test_data.txt\"\n",
    "    train_file = \"../Data/docstring_training_data.txt\"\n",
    "    base_output_dir = \"../Models/\"\n",
    "    print(\"The notebook is running locally.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:37.452132Z",
     "end_time": "2024-12-27T19:24:37.459441Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST (File size 6673607 bytes):\n",
      "[Function] def shortest_dist(dist_mat):\\n    (m, n) = dist_mat.size()[:2]\\n    dist = [[0 for _ in range(n)] for _ in range(m)]\\n    for i in range(m):\\n        for j in range(n):\\n            if i == 0 and j == 0:\\n                dist[i][j] = dist_mat[i, j]\\n            elif i == 0 and j > 0:\\n                dist[i][j] = dist[i][j - 1] + dist_mat[i, j]\\n            elif i > 0 and j == 0:\\n                dist[i][j] = dist[i - 1][j] + dist_mat[i, j]\\n            else:\\n                dist[i][j] = torch.min(dist[i - 1][j], dist[i][j - 1]) + dist_mat[i, j]\\n    dist = dist[-1][-1]\\n    return dist [Docstring] Parallel version.\\nArgs:\\n  dist_mat: pytorch Variable, available shape:\\n    1) [m, n]\\n    2) [m, n, N], N is batch size\\n    3) [m, n, *], * can be arbitrary additional dimensions\\nReturns:\\n  dist: three cases corresponding to `dist_mat`:\\n    1) scalar\\n    2) pytorch Variable, with shape [N]\\n    3) pytorch Variable, with shape [*] [EOS]\n",
      "\n",
      "[Function] def _construct_linear_audio_network(include_frontend=True):\\n    weight_decay = 1e-05\\n    n_dft = 512\\n    n_hop = 242\\n    asr = 48000\\n    audio_window_dur = 1\\n    if include_frontend:\\n        input_shape = (1, asr * audio_window_dur)\\n        x_a = Input(shape=input_shape, dtype='float32')\\n        from kapre.composed import get_stft_magnitude_layer\\n        spec = __fix_kapre_spec(get_stft_magnitude_layer)(input_shape=input_shape, n_fft=n_dft, hop_length=n_hop, return_decibel=True, input_data_format='channels_first', output_data_format='channels_last')\\n        y_a = spec(x_a)\\n    else:\\n        input_shape = (n_dft // 2 + 1, int(np.ceil((asr - n_dft) * audio_window_dur / n_hop)), 1)\\n        x_a = y_a = Input(shape=input_shape, dtype='float32')\\n    y_a = BatchNormalization()(y_a)\\n    n_filter_a_1 = 64\\n    filt_size_a_1 = (3, 3)\\n    pool_size_a_1 = (2, 2)\\n    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\\n    n_filter_a_2 = 128\\n    filt_size_a_2 = (3, 3)\\n    pool_size_a_2 = (2, 2)\\n    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\\n    n_filter_a_3 = 256\\n    filt_size_a_3 = (3, 3)\\n    pool_size_a_3 = (2, 2)\\n    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\\n    n_filter_a_4 = 512\\n    filt_size_a_4 = (3, 3)\\n    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_4, filt_size_a_4, kernel_initializer='he_normal', name='audio_embedding_layer', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    m = Model(inputs=x_a, outputs=y_a)\\n    return m [Docstring] Returns an uninitialized model object for an audio network with a linear\\nspectrogram input (With 257 frequency bins)\\n\\nReturns\\n-------\\nmodel : tf.keras.Model\\n    Model object. [EOS]\n",
      "\n",
      "[Function] def _expect_true(self, response):\\n    if response.text == u'true':\\n        return True\\n    raise BitstampError('Unexpected response') [Docstring] A shortcut that raises a :class:`BitstampError` if the response didn't\\njust contain the text 'true'. [EOS]\n",
      "\n",
      "TRAIN (File size 15639721 bytes):\n",
      "[Function] def squeeze_and_excite(inputs, filters, ratio, num_frames):\\n    reduced_filters = max(int(filters * ratio), 8)\\n    feature_shape = [inputs.shape[0], 1, 1, inputs.shape[3]]\\n    squeeze_excite_inputs = tf.reshape(inputs, [int(tf.compat.dimension_value(inputs.shape[0]) // num_frames), num_frames * inputs.shape[1], inputs.shape[2], -1])\\n    squeeze_excite = tf.reduce_mean(squeeze_excite_inputs, [1, 2], keepdims=True)\\n    squeeze_excite = tf.layers.conv2d(inputs=squeeze_excite, filters=reduced_filters, kernel_size=1, strides=1, padding='SAME', use_bias=True, kernel_initializer=contrib_layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False), data_format='channels_last')\\n    squeeze_excite = tf.layers.conv2d(inputs=tf.nn.relu(squeeze_excite), filters=filters, kernel_size=1, strides=1, padding='SAME', use_bias=True, kernel_initializer=contrib_layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False), data_format='channels_last')\\n    squeeze_excite = tf.expand_dims(tf.nn.sigmoid(squeeze_excite), 1)\\n    pattern = tf.stack([1, num_frames, 1, 1, 1])\\n    return tf.reshape(tf.tile(squeeze_excite, pattern), feature_shape) * inputs [Docstring] Squeeze and excite layer for videos.\\n\\nSqueeze-and-Excitation Networks\\narXiv: 1709.01507\\n\\nArgs:\\n  inputs: `Tensor` of size `[batch*time, height, width, channels]`. Only\\n    supports 'channels_last' as the data format.\\n  filters: `int` number of filters in the convolution.\\n  ratio: 'float' percent to squeeze\\n  num_frames: int number of frames\\n\\nReturns:\\n  A `Tensor` of the same data_format [EOS]\n",
      "\n",
      "[Function] def CRNN2D(X_shape, nb_classes):\\n    nb_layers = 4\\n    nb_filters = [64, 128, 128, 128]\\n    kernel_size = (3, 3)\\n    activation = 'elu'\\n    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2), (4, 2)]\\n    input_shape = (X_shape[1], X_shape[2], X_shape[3])\\n    frequency_axis = 1\\n    time_axis = 2\\n    channel_axis = 3\\n    model = Sequential()\\n    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\\n    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same', data_format='channels_last', input_shape=input_shape))\\n    model.add(Activation(activation))\\n    model.add(BatchNormalization(axis=channel_axis))\\n    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\\n    model.add(Dropout(0.1))\\n    for layer in range(nb_layers - 1):\\n        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size, padding='same'))\\n        model.add(Activation(activation))\\n        model.add(BatchNormalization(axis=channel_axis))\\n        model.add(MaxPooling2D(pool_size=pool_size[layer + 1], strides=pool_size[layer + 1]))\\n        model.add(Dropout(0.1))\\n    model.add(Permute((time_axis, frequency_axis, channel_axis)))\\n    resize_shape = model.output_shape[2] * model.output_shape[3]\\n    model.add(Reshape((model.output_shape[1], resize_shape)))\\n    model.add(GRU(32, return_sequences=True))\\n    model.add(GRU(32, return_sequences=False))\\n    model.add(Dropout(0.3))\\n    model.add(Dense(nb_classes))\\n    model.add(Activation('softmax'))\\n    return model [Docstring] Model used for evaluation in paper. Inspired by K. Choi model in:\\nhttps://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py [EOS]\n",
      "\n",
      "[Function] def _validate_label_map(label_map):\\n    for item in label_map.item:\\n        if item.id < 1:\\n            raise ValueError('Label map ids should be >= 1.') [Docstring] Checks if a label map is valid.\\n\\nArgs:\\n  label_map: StringIntLabelMap to validate.\\n\\nRaises:\\n  ValueError: if label map is invalid. [EOS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def file_ready(filepath, min_size):\n",
    "    \"\"\"Check whether the file exists and exceeds a minimum size.\"\"\"\n",
    "    return os.path.isfile(filepath) and os.path.getsize(filepath) >= min_size\n",
    "\n",
    "\n",
    "timeout = 60\n",
    "min_size = 42\n",
    "start_time = time.time()\n",
    "\n",
    "while not (file_ready(test_file, min_size) and file_ready(train_file, min_size)):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > timeout:\n",
    "        print(\"Timeout\")\n",
    "        break\n",
    "else:\n",
    "    print(\"TEST (File size\", os.path.getsize(test_file), \"bytes):\")\n",
    "    with open(test_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line)\n",
    "            if i == 2: break\n",
    "\n",
    "    print(\"TRAIN (File size\", os.path.getsize(train_file), \"bytes):\")\n",
    "    with open(train_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line)\n",
    "            if i == 2: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:43.963597Z",
     "end_time": "2024-12-27T19:24:43.969074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access token loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load API-Token from .env\n",
    "load_dotenv()\n",
    "\n",
    "huggingface_api_token = os.getenv(\"HUGGINGFACE\")\n",
    "wandb_api_token = os.getenv(\"WANDB\")\n",
    "if huggingface_api_token and wandb_api_token:\n",
    "    print(\"Access token loaded.\")\n",
    "else:\n",
    "    print(\"Access token not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:46.944075Z",
     "end_time": "2024-12-27T19:24:46.951250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_dataset_from_text_files(train_file_path, test_file_path):\n",
    "    \"\"\"\n",
    "    Load training and test datasets from text files into a DatasetDict.\n",
    "\n",
    "    Args:\n",
    "        train_file_path (str): Path to the training data text file.\n",
    "        test_file_path (str): Path to the test data text file.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A dictionary containing 'train' and 'test' datasets with text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_lines_from_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        print(f'Total lines loaded from {file_path}: {len(lines)}')\n",
    "        return lines\n",
    "\n",
    "    def get_dataset_generator(file_path):\n",
    "        lines = get_lines_from_file(file_path)\n",
    "        for line in lines:\n",
    "            yield {\"text\": line}\n",
    "\n",
    "    dataset_train = Dataset.from_generator(\n",
    "        generator=lambda: get_dataset_generator(train_file_path),\n",
    "        features=Features({'text': Value('string')})\n",
    "    )\n",
    "    dataset_test = Dataset.from_generator(\n",
    "        generator=lambda: get_dataset_generator(test_file_path),\n",
    "        features=Features({'text': Value('string')})\n",
    "    )\n",
    "    return DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
    "\n",
    "\n",
    "datasets = load_dataset_from_text_files(train_file, test_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:52.806511Z",
     "end_time": "2024-12-27T19:24:52.895013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running locally: Using bfloat16 precision.\n"
     ]
    }
   ],
   "source": [
    "# Model selection and configuration\n",
    "model_to_finetune = \"meta-llama/CodeLlama-7b-Python-hf\"\n",
    "# model_to_finetune = \"tiiuae/falcon-rw-1b\"\n",
    "# model_to_finetune = \"tiiuae/falcon-7b\"\n",
    "\n",
    "if on_colab:\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    torch_dtype = None\n",
    "    device_map = \"auto\"\n",
    "    print(\"Notebook is running on Colab: Using 4-bit quantization.\")\n",
    "else:\n",
    "    quantization_config = None\n",
    "    torch_dtype = torch.bfloat16\n",
    "    device_map = \"cpu\"\n",
    "    print(\"Notebook is running locally: Using bfloat16 precision.\")\n",
    "\n",
    "# Load tokeniser and model with auth token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_to_finetune,\n",
    "    token=huggingface_api_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_to_finetune,\n",
    "    token=huggingface_api_token,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Deactivating cache & setting pretraining\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:24:55.056516Z",
     "end_time": "2024-12-27T19:24:56.788389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example 918 : {'text': \"[Function] def _is_whitespace(char):\\\\n    if char == ' ' or char == '\\\\t' or char == '\\\\n' or (char == '\\\\r'):\\\\n        return True\\\\n    cat = unicodedata.category(char)\\\\n    if cat == 'Zs':\\\\n        return True\\\\n    return False [Docstring] Checks whether `chars` is a whitespace character.\", 'input_ids': [58, 22203, 60, 825, 4808, 271, 62, 1929, 2737, 10223, 7, 10641, 2599, 59, 77, 220, 220, 220, 611, 1149, 6624, 705, 705, 393, 1149, 6624, 705, 59, 83, 6, 393, 1149, 6624, 705, 59, 77, 6, 393, 357, 10641, 6624, 705, 59, 81, 6, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 1441, 6407, 59, 77, 220, 220, 220, 3797, 796, 28000, 9043, 1045, 13, 22872, 7, 10641, 19415, 77, 220, 220, 220, 611, 3797, 6624, 705, 57, 82, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 1441, 6407, 59, 77, 220, 220, 220, 1441, 10352, 685, 23579, 8841, 60, 47719, 1771, 4600, 354, 945, 63, 318, 257, 13216, 10223, 2095, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 7232 : {'text': '[Function] def all_words_in_sentences(sentences):\\\\n    vocab = []\\\\n    if isinstance(sentences, WordGenerator):\\\\n        sentences = [s for (s, _) in sentences]\\\\n    for sentence in sentences:\\\\n        for word in sentence:\\\\n            if word not in vocab:\\\\n                vocab.append(word)\\\\n    return vocab [Docstring] Extracts all unique words from a given list of sentences.\\\\n\\\\n# Arguments:\\\\n    sentences: List or word generator of sentences to be processed.\\\\n\\\\n# Returns:\\\\n    List of all unique words contained in the given sentences.', 'input_ids': [58, 22203, 60, 825, 477, 62, 10879, 62, 259, 62, 34086, 3007, 7, 34086, 3007, 2599, 59, 77, 220, 220, 220, 12776, 397, 796, 17635, 59, 77, 220, 220, 220, 611, 318, 39098, 7, 34086, 3007, 11, 9678, 8645, 1352, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 13439, 796, 685, 82, 329, 357, 82, 11, 4808, 8, 287, 13439, 60, 59, 77, 220, 220, 220, 329, 6827, 287, 13439, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 329, 1573, 287, 6827, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 1573, 407, 287, 12776, 397, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 12776, 397, 13, 33295, 7, 4775, 19415, 77, 220, 220, 220, 1441, 12776, 397, 685, 23579, 8841, 60, 29677, 82, 477, 3748, 2456, 422, 257, 1813, 1351, 286, 13439, 13, 59, 77, 59, 77, 2, 20559, 2886, 7479, 77, 220, 220, 220, 13439, 25, 7343, 393, 1573, 17301, 286, 13439, 284, 307, 13686, 13, 59, 77, 59, 77, 2, 16409, 7479, 77, 220, 220, 220, 7343, 286, 477, 3748, 2456, 7763, 287, 262, 1813, 13439, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 2784 : {'text': '[Function] def LocalSearchICML2020(dataset, *, k, dist_threshold_vec, coeff_anchor=3.0, coeff_search=1.0, epsilon=0.01, use_lloyd=False):\\\\n    (anchor_points, centers) = AnchorPointInitialization(dataset, k, dist_threshold_vec, coeff_anchor=coeff_anchor)\\\\n    curr_cost = fair_clustering_utils.KMeansCost(dataset, [dataset[c] for c in centers])\\\\n    t2cc = fair_clustering_utils.TopTwoClosestToCenters(dataset, centers)\\\\n    iteration = 0\\\\n    while True:\\\\n        iteration += 1\\\\n        found = False\\\\n        proposed_new_center = 0\\\\n        while not found and proposed_new_center < len(dataset):\\\\n            if proposed_new_center in centers:\\\\n                proposed_new_center += 1\\\\n                continue\\\\n            for proposed_to_remove in centers:\\\\n                if IsValidCenterSwap(dataset, anchor_points, centers, coeff_search * dist_threshold_vec, proposed_to_remove, proposed_new_center):\\\\n                    c = t2cc.CostAfterSwap(proposed_to_remove, proposed_new_center)\\\\n                    if c < curr_cost * (1 - epsilon):\\\\n                        centers.remove(proposed_to_remove)\\\\n                        centers.add(proposed_new_center)\\\\n                        t2cc.SwapCenters(proposed_to_remove, proposed_new_center)\\\\n                        curr_cost = c\\\\n                        found = True\\\\n                        break\\\\n            proposed_new_center += 1\\\\n        if not found:\\\\n            break\\\\n    assert len(centers) == k\\\\n    centers = [dataset[c] for c in centers]\\\\n    if use_lloyd:\\\\n        return fair_clustering_utils.LloydImprovement(dataset, anchor_points_pos=anchor_points, inital_centers_vec=centers, dist_threshold_vec=dist_threshold_vec * 3, num_iter=20)\\\\n    else:\\\\n        return centers [Docstring] Implementation of the baseline individually-fair clustering algorithm.\\\\n\\\\nThe algorithm outputs an approximate bicreteria k-means solution under\\\\nindividual-fairness constraints. The algorithm was presented in\\\\n\"Individual fairness for k-clustering\", Mahabadi, Vakilian, ICML 2020.\\\\n\\\\nArgs:\\\\n  dataset: the set of points.\\\\n  k: the number of clusters.\\\\n  dist_threshold_vec: the individual fairness distance thresholds of the\\\\n    points.\\\\n  coeff_anchor: coefficent used for the anchor points distance threshold.\\\\n  coeff_search: coefficent used for the local search distance threshold.\\\\n  epsilon: improvement required for a swap.\\\\n  use_lloyd: whether to use the fair Lloyd improvement steps. Note: Lloyd was\\\\n    not part of the ICML2020 paper.\\\\n\\\\nReturns:\\\\n  The cluster centers.\\\\n\\\\nRaises:\\\\n  RuntimeError: if the solution is not feasible.', 'input_ids': [58, 22203, 60, 825, 10714, 18243, 2149, 5805, 42334, 7, 19608, 292, 316, 11, 1635, 11, 479, 11, 1233, 62, 400, 10126, 62, 35138, 11, 763, 14822, 62, 3702, 273, 28, 18, 13, 15, 11, 763, 14822, 62, 12947, 28, 16, 13, 15, 11, 304, 862, 33576, 28, 15, 13, 486, 11, 779, 62, 297, 12192, 28, 25101, 2599, 59, 77, 220, 220, 220, 357, 3702, 273, 62, 13033, 11, 10399, 8, 796, 29253, 273, 12727, 24243, 1634, 7, 19608, 292, 316, 11, 479, 11, 1233, 62, 400, 10126, 62, 35138, 11, 763, 14822, 62, 3702, 273, 28, 1073, 14822, 62, 3702, 273, 19415, 77, 220, 220, 220, 1090, 81, 62, 15805, 796, 3148, 62, 565, 436, 1586, 62, 26791, 13, 42, 5308, 504, 13729, 7, 19608, 292, 316, 11, 685, 19608, 292, 316, 58, 66, 60, 329, 269, 287, 10399, 12962, 59, 77, 220, 220, 220, 256, 17, 535, 796, 3148, 62, 565, 436, 1586, 62, 26791, 13, 9126, 7571, 2601, 418, 395, 2514, 19085, 364, 7, 19608, 292, 316, 11, 10399, 19415, 77, 220, 220, 220, 24415, 796, 657, 59, 77, 220, 220, 220, 981, 6407, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 24415, 15853, 352, 59, 77, 220, 220, 220, 220, 220, 220, 220, 1043, 796, 10352, 59, 77, 220, 220, 220, 220, 220, 220, 220, 5150, 62, 3605, 62, 16159, 796, 657, 59, 77, 220, 220, 220, 220, 220, 220, 220, 981, 407, 1043, 290, 5150, 62, 3605, 62, 16159, 1279, 18896, 7, 19608, 292, 316, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 5150, 62, 3605, 62, 16159, 287, 10399, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5150, 62, 3605, 62, 16159, 15853, 352, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2555, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 329, 5150, 62, 1462, 62, 28956, 287, 10399, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 1148, 47139, 23656, 10462, 499, 7, 19608, 292, 316, 11, 18021, 62, 13033, 11, 10399, 11, 763, 14822, 62, 12947, 1635, 1233, 62, 400, 10126, 62, 35138, 11, 5150, 62, 1462, 62, 28956, 11, 5150, 62, 3605, 62, 16159, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 269, 796, 256, 17, 535, 13, 13729, 3260, 10462, 499, 7, 22930, 1335, 62, 1462, 62, 28956, 11, 5150, 62, 3605, 62, 16159, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 269, 1279, 1090, 81, 62, 15805, 1635, 357, 16, 532, 304, 862, 33576, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 10399, 13, 28956, 7, 22930, 1335, 62, 1462, 62, 28956, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 10399, 13, 2860, 7, 22930, 1335, 62, 3605, 62, 16159, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 256, 17, 535, 13, 10462, 499, 19085, 364, 7, 22930, 1335, 62, 1462, 62, 28956, 11, 5150, 62, 3605, 62, 16159, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1090, 81, 62, 15805, 796, 269, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1043, 796, 6407, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2270, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5150, 62, 3605, 62, 16159, 15853, 352, 59, 77, 220, 220, 220, 220, 220, 220, 220, 611, 407, 1043, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2270, 59, 77, 220, 220, 220, 6818, 18896, 7, 1087, 364, 8, 6624, 479, 59, 77, 220, 220, 220, 10399, 796, 685, 19608, 292, 316, 58, 66, 60, 329, 269, 287, 10399, 60, 59, 77, 220, 220, 220, 611, 779, 62, 297, 12192, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 1441, 3148, 62, 565, 436, 1586, 62, 26791, 13, 43, 75, 12192, 47531, 434, 7, 19608, 292, 316, 11, 18021, 62, 13033, 62, 1930, 28, 3702, 273, 62, 13033, 11, 287, 1287, 62, 1087, 364, 62, 35138, 28, 1087, 364, 11, 1233, 62, 400, 10126, 62, 35138, 28, 17080, 62, 400, 10126, 62, 35138, 1635, 513, 11, 997, 62, 2676, 28, 1238, 19415, 77, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 1441, 10399, 685, 23579, 8841, 60, 46333, 286, 262, 14805, 17033, 12, 22043, 32966, 1586, 11862, 13, 59, 77, 59, 77, 464, 11862, 23862, 281, 27665, 275, 291, 260, 353, 544, 479, 12, 1326, 504, 4610, 739, 59, 77, 43129, 12, 22043, 1108, 17778, 13, 383, 11862, 373, 5545, 287, 59, 77, 1, 35392, 22692, 329, 479, 12, 565, 436, 1586, 1600, 8882, 397, 9189, 11, 569, 461, 35824, 11, 12460, 5805, 12131, 13, 59, 77, 59, 77, 42035, 7479, 77, 220, 27039, 25, 262, 900, 286, 2173, 13, 59, 77, 220, 479, 25, 262, 1271, 286, 23163, 13, 59, 77, 220, 1233, 62, 400, 10126, 62, 35138, 25, 262, 1981, 22692, 5253, 40885, 286, 262, 59, 77, 220, 220, 220, 2173, 13, 59, 77, 220, 763, 14822, 62, 3702, 273, 25, 763, 24531, 298, 973, 329, 262, 18021, 2173, 5253, 11387, 13, 59, 77, 220, 763, 14822, 62, 12947, 25, 763, 24531, 298, 973, 329, 262, 1957, 2989, 5253, 11387, 13, 59, 77, 220, 304, 862, 33576, 25, 9025, 2672, 329, 257, 16075], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 556 : {'text': \"[Function] def resize_image(image, min_dim=None, max_dim=None, min_scale=None, mode='square'):\\\\n    image_dtype = image.dtype\\\\n    (h, w) = image.shape[:2]\\\\n    window = (0, 0, h, w)\\\\n    scale = 1\\\\n    padding = [(0, 0), (0, 0), (0, 0)]\\\\n    crop = None\\\\n    if mode == 'none':\\\\n        return (image, window, scale, padding, crop)\\\\n    if min_dim:\\\\n        scale = max(1, min_dim / min(h, w))\\\\n    if min_scale and scale < min_scale:\\\\n        scale = min_scale\\\\n    if max_dim and mode == 'square':\\\\n        image_max = max(h, w)\\\\n        if round(image_max * scale) > max_dim:\\\\n            scale = max_dim / image_max\\\\n    if scale != 1:\\\\n        image = resize(image, (round(h * scale), round(w * scale)), preserve_range=True)\\\\n    if mode == 'square':\\\\n        (h, w) = image.shape[:2]\\\\n        top_pad = (max_dim - h) // 2\\\\n        bottom_pad = max_dim - h - top_pad\\\\n        left_pad = (max_dim - w) // 2\\\\n        right_pad = max_dim - w - left_pad\\\\n        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\\\\n        image = np.pad(image, padding, mode='constant', constant_values=0)\\\\n        window = (top_pad, left_pad, h + top_pad, w + left_pad)\\\\n    elif mode == 'pad64':\\\\n        (h, w) = image.shape[:2]\\\\n        assert min_dim % 64 == 0, 'Minimum dimension must be a multiple of 64'\\\\n        if h % 64 > 0:\\\\n            max_h = h - h % 64 + 64\\\\n            top_pad = (max_h - h) // 2\\\\n            bottom_pad = max_h - h - top_pad\\\\n        else:\\\\n            top_pad = bottom_pad = 0\\\\n        if w % 64 > 0:\\\\n            max_w = w - w % 64 + 64\\\\n            left_pad = (max_w - w) // 2\\\\n            right_pad = max_w - w - left_pad\\\\n        else:\\\\n            left_pad = right_pad = 0\\\\n        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\\\\n        image = np.pad(image, padding, mode='constant', constant_values=0)\\\\n        window = (top_pad, left_pad, h + top_pad, w + left_pad)\\\\n    elif mode == 'crop':\\\\n        (h, w) = image.shape[:2]\\\\n        y = random.randint(0, h - min_dim)\\\\n        x = random.randint(0, w - min_dim)\\\\n        crop = (y, x, min_dim, min_dim)\\\\n        image = image[y:y + min_dim, x:x + min_dim]\\\\n        window = (0, 0, min_dim, min_dim)\\\\n    else:\\\\n        raise Exception('Mode {} not supported'.format(mode))\\\\n    return (image.astype(image_dtype), window, scale, padding, crop) [Docstring] Resizes an image keeping the aspect ratio unchanged.\\\\n\\\\nmin_dim: if provided, resizes the image such that it's smaller\\\\n    dimension == min_dim\\\\nmax_dim: if provided, ensures that the image longest side doesn't\\\\n    exceed this value.\\\\nmin_scale: if provided, ensure that the image is scaled up by at least\\\\n    this percent even if min_dim doesn't require it.\\\\nmode: Resizing mode.\\\\n    none: No resizing. Return the image unchanged.\\\\n    square: Resize and pad with zeros to get a square image\\\\n        of size [max_dim, max_dim].\\\\n    pad64: Pads width and height with zeros to make them multiples of 64.\\\\n           If min_dim or min_scale are provided, it scales the image up\\\\n           before padding. max_dim is ignored in this mode.\\\\n           The multiple of 64 is needed to ensure smooth scaling of feature\\\\n           maps up and down the 6 levels of the FPN pyramid (2**6=64).\\\\n    crop: Picks random crops from the image. First, scales the image based\\\\n          on min_dim and min_scale, then picks a random crop of\\\\n          size min_dim x min_dim. Can be used in training only.\\\\n          max_dim is not used in this mode.\\\\n\\\\nReturns:\\\\nimage: the resized image\\\\nwindow: (y1, x1, y2, x2). If max_dim is provided, padding might\\\\n    be inserted in the returned image. If so, this window is the\\\\n    coordinates of the image part of the full image (excluding\\\\n    the padding). The x2, y2 pixels are not included.\\\\nscale: The scale factor used to resize the image\\\\npadding: Padding added to the image [(top, bottom), (left, right), (0, 0)]\", 'input_ids': [58, 22203, 60, 825, 47558, 62, 9060, 7, 9060, 11, 949, 62, 27740, 28, 14202, 11, 3509, 62, 27740, 28, 14202, 11, 949, 62, 9888, 28, 14202, 11, 4235, 11639, 23415, 6, 2599, 59, 77, 220, 220, 220, 2939, 62, 67, 4906, 796, 2939, 13, 67, 4906, 59, 77, 220, 220, 220, 357, 71, 11, 266, 8, 796, 2939, 13, 43358, 58, 25, 17, 60, 59, 77, 220, 220, 220, 4324, 796, 357, 15, 11, 657, 11, 289, 11, 266, 19415, 77, 220, 220, 220, 5046, 796, 352, 59, 77, 220, 220, 220, 24511, 796, 47527, 15, 11, 657, 828, 357, 15, 11, 657, 828, 357, 15, 11, 657, 15437, 59, 77, 220, 220, 220, 13833, 796, 6045, 59, 77, 220, 220, 220, 611, 4235, 6624, 705, 23108, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 1441, 357, 9060, 11, 4324, 11, 5046, 11, 24511, 11, 13833, 19415, 77, 220, 220, 220, 611, 949, 62, 27740, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 5046, 796, 3509, 7, 16, 11, 949, 62, 27740, 1220, 949, 7, 71, 11, 266, 4008, 59, 77, 220, 220, 220, 611, 949, 62, 9888, 290, 5046, 1279, 949, 62, 9888, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 5046, 796, 949, 62, 9888, 59, 77, 220, 220, 220, 611, 3509, 62, 27740, 290, 4235, 6624, 705, 23415, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 2939, 62, 9806, 796, 3509, 7, 71, 11, 266, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 611, 2835, 7, 9060, 62, 9806, 1635, 5046, 8, 1875, 3509, 62, 27740, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5046, 796, 3509, 62, 27740, 1220, 2939, 62, 9806, 59, 77, 220, 220, 220, 611, 5046, 14512, 352, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 2939, 796, 47558, 7, 9060, 11, 357, 744, 7, 71, 1635, 5046, 828, 2835, 7, 86, 1635, 5046, 36911, 12201, 62, 9521, 28, 17821, 19415, 77, 220, 220, 220, 611, 4235, 6624, 705, 23415, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 357, 71, 11, 266, 8, 796, 2939, 13, 43358, 58, 25, 17, 60, 59, 77, 220, 220, 220, 220, 220, 220, 220, 1353, 62, 15636, 796, 357, 9806, 62, 27740, 532, 289, 8, 3373, 362, 59, 77, 220, 220, 220, 220, 220, 220, 220, 4220, 62, 15636, 796, 3509, 62, 27740, 532, 289, 532, 1353, 62, 15636, 59, 77, 220, 220, 220, 220, 220, 220, 220, 1364, 62, 15636, 796, 357, 9806, 62, 27740, 532, 266, 8, 3373, 362, 59, 77, 220, 220, 220, 220, 220, 220, 220, 826, 62, 15636, 796, 3509, 62, 27740, 532, 266, 532, 1364, 62, 15636, 59, 77, 220, 220, 220, 220, 220, 220, 220, 24511, 796, 47527, 4852, 62, 15636, 11, 4220, 62, 15636, 828, 357, 9464, 62, 15636, 11, 826, 62, 15636, 828, 357, 15, 11, 657, 15437, 59, 77, 220, 220, 220, 220, 220, 220, 220, 2939, 796, 45941, 13, 15636, 7, 9060, 11, 24511, 11, 4235, 11639, 9979, 415, 3256, 6937, 62, 27160, 28, 15, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 4324, 796, 357, 4852, 62, 15636, 11, 1364, 62, 15636, 11, 289, 1343, 1353, 62, 15636, 11, 266, 1343, 1364, 62, 15636, 19415, 77, 220, 220, 220, 1288, 361, 4235, 6624, 705, 15636, 2414, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 357, 71, 11, 266, 8, 796, 2939, 13, 43358, 58, 25, 17, 60, 59, 77, 220, 220, 220, 220, 220, 220, 220, 6818, 949, 62, 27740, 4064, 5598, 6624, 657, 11, 705, 44046, 15793, 1276, 307, 257, 3294, 286, 5598, 6, 59, 77, 220, 220, 220, 220, 220, 220, 220, 611, 289, 4064, 5598, 1875, 657, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3509, 62, 71, 796, 289, 532, 289, 4064, 5598, 1343, 5598, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1353, 62, 15636, 796, 357, 9806, 62, 71, 532, 289, 8, 3373, 362, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 4220, 62, 15636, 796, 3509, 62, 71, 532, 289, 532, 1353, 62, 15636, 59, 77, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1353, 62, 15636, 796, 4220, 62, 15636, 796, 657, 59, 77, 220, 220, 220, 220, 220, 220, 220, 611, 266, 4064, 5598, 1875, 657, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3509, 62, 86, 796, 266, 532, 266, 4064, 5598, 1343, 5598, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1364, 62, 15636, 796, 357, 9806, 62, 86, 532, 266, 8, 3373, 362, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 826, 62, 15636, 796, 3509, 62, 86, 532, 266, 532, 1364, 62, 15636, 59, 77, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1364, 62, 15636, 796, 826, 62, 15636, 796, 657, 59, 77, 220, 220, 220, 220, 220, 220, 220, 24511, 796, 47527, 4852, 62, 15636, 11, 4220, 62, 15636, 828, 357, 9464, 62, 15636, 11, 826, 62, 15636, 828, 357, 15, 11, 657, 15437, 59, 77, 220, 220, 220, 220, 220, 220, 220, 2939, 796, 45941, 13, 15636, 7, 9060, 11, 24511, 11, 4235, 11639, 9979, 415, 3256, 6937, 62, 27160, 28, 15, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 4324, 796, 357, 4852, 62, 15636, 11, 1364, 62, 15636, 11, 289, 1343, 1353, 62, 15636, 11, 266, 1343, 1364, 62, 15636, 19415, 77, 220, 220, 220, 1288, 361, 4235, 6624, 705, 31476, 6, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 357, 71, 11, 266, 8, 796, 2939, 13, 43358, 58, 25, 17, 60, 59, 77, 220, 220, 220, 220, 220, 220, 220, 331, 796, 4738, 13, 25192, 600, 7, 15, 11, 289, 532, 949, 62, 27740, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 2124, 796, 4738, 13, 25192, 600, 7, 15, 11], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 8535 : {'text': '[Function] def example_kwargs(dl_args, cache_path=None, absolute_path=True, dry_run=False):\\\\n    example_files = {}\\\\n    for (k, v) in dl_args.items():\\\\n        if isinstance(v.example, UNSPECIFIED):\\\\n            continue\\\\n        if isinstance(v.example, RemoteFile) and cache_path is not None:\\\\n            if absolute_path:\\\\n                dl_dir = os.path.abspath(cache_path)\\\\n            else:\\\\n                dl_dir = cache_path\\\\n            if not os.path.exists(dl_dir):\\\\n                os.makedirs(dl_dir)\\\\n            if v.example.name != \\'\\':\\\\n                path = os.path.join(dl_dir, v.example.name)\\\\n            else:\\\\n                path = os.path.join(dl_dir, k)\\\\n            example_files[k] = path\\\\n            if os.path.exists(path):\\\\n                if v.example.validate(path):\\\\n                    logger.info(\\'Example file for argument {} already exists\\'.format(k))\\\\n                else:\\\\n                    logger.info(\"Example file for argument {} doesn\\'t match the md5 hash {}. Re-downloading\".format(k, v.example.md5))\\\\n                    if not dry_run:\\\\n                        v.example.get_file(path)\\\\n            elif not dry_run:\\\\n                v.example.get_file(path)\\\\n        else:\\\\n            example_files[k] = v.example\\\\n    return example_files [Docstring] Return the example kwargs.\\\\n\\\\nArgs:\\\\n  dl_args: dictionary of dataloader args\\\\n  cache_path: if specified, save the examples to that directory', 'input_ids': [58, 22203, 60, 825, 1672, 62, 46265, 22046, 7, 25404, 62, 22046, 11, 12940, 62, 6978, 28, 14202, 11, 4112, 62, 6978, 28, 17821, 11, 5894, 62, 5143, 28, 25101, 2599, 59, 77, 220, 220, 220, 1672, 62, 16624, 796, 23884, 59, 77, 220, 220, 220, 329, 357, 74, 11, 410, 8, 287, 288, 75, 62, 22046, 13, 23814, 33529, 59, 77, 220, 220, 220, 220, 220, 220, 220, 611, 318, 39098, 7, 85, 13, 20688, 11, 4725, 48451, 28343, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2555, 59, 77, 220, 220, 220, 220, 220, 220, 220, 611, 318, 39098, 7, 85, 13, 20688, 11, 21520, 8979, 8, 290, 12940, 62, 6978, 318, 407, 6045, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 4112, 62, 6978, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 288, 75, 62, 15908, 796, 28686, 13, 6978, 13, 397, 2777, 776, 7, 23870, 62, 6978, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 288, 75, 62, 15908, 796, 12940, 62, 6978, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 407, 28686, 13, 6978, 13, 1069, 1023, 7, 25404, 62, 15908, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 28686, 13, 76, 4335, 17062, 7, 25404, 62, 15908, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 410, 13, 20688, 13, 3672, 14512, 10148, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3108, 796, 28686, 13, 6978, 13, 22179, 7, 25404, 62, 15908, 11, 410, 13, 20688, 13, 3672, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3108, 796, 28686, 13, 6978, 13, 22179, 7, 25404, 62, 15908, 11, 479, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1672, 62, 16624, 58, 74, 60, 796, 3108, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 28686, 13, 6978, 13, 1069, 1023, 7, 6978, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 410, 13, 20688, 13, 12102, 378, 7, 6978, 2599, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 49706, 13, 10951, 10786, 16281, 2393, 329, 4578, 23884, 1541, 7160, 4458, 18982, 7, 74, 4008, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 49706, 13, 10951, 7203, 16281, 2393, 329, 4578, 23884, 1595, 470, 2872, 262, 45243, 20, 12234, 23884, 13, 797, 12, 15002, 278, 1911, 18982, 7, 74, 11, 410, 13, 20688, 13, 9132, 20, 4008, 59, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 407, 5894, 62, 5143, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 410, 13, 20688, 13, 1136, 62, 7753, 7, 6978, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1288, 361, 407, 5894, 62, 5143, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 410, 13, 20688, 13, 1136, 62, 7753, 7, 6978, 19415, 77, 220, 220, 220, 220, 220, 220, 220, 2073, 7479, 77, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1672, 62, 16624, 58, 74, 60, 796, 410, 13, 20688, 59, 77, 220, 220, 220, 1441, 1672, 62, 16624, 685, 23579, 8841, 60, 8229, 262, 1672, 479, 86, 22046, 13, 59, 77, 59, 77, 42035, 7479, 77, 220, 288, 75, 62, 22046, 25, 22155, 286, 4818, 282, 1170, 263, 26498, 59, 77, 220, 12940, 62, 6978, 25, 611, 7368, 11, 3613, 262, 6096, 284, 326, 8619], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation of the data sets for training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Example output of some training examples\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(tokenized_datasets[\"train\"]) - 1)\n",
    "    print(\"Train example\", index, \":\", tokenized_datasets[\"train\"][index])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:25:01.276829Z",
     "end_time": "2024-12-27T19:25:01.327124Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 - Response:\n",
      " Write a docstring for the following Python code:\n",
      " [Function]\n",
      "def add_numbers(a, b): return a + b \n",
      " [Docstring]\n",
      "def add_numbers(a, b): return a + b\n",
      "The docstring for add_numbers looks like this:\n",
      "def add_numbers(a, b): return a + b\n",
      "The docstring is the body of the function. The first part of the docstring is the function name (add_numbers). The second part of the docstring is the body of the function, which can be any Python code you want.\n",
      "You can also add a description\n",
      "Prompt 2 - Response:\n",
      " Write a docstring for the following Python code:\n",
      " [Function]\n",
      "def subtract_numbers(a, b): return a - b \n",
      " [Docstring]\n",
      "def subtract_numbers(a, b): return a - b\n",
      "The docstring should have the following structure:\n",
      "[Docstring]\n",
      "def subtract_numbers(a, b):\n",
      "return a - b\n",
      "The docstring for the function should be written in the following format:\n",
      "[Docstring]\n",
      "def subtract_numbers(a, b):\n",
      "return a - b\n",
      "The docstring for the function should have a single line per function definition with no indentation\n"
     ]
    }
   ],
   "source": [
    "def test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Tests the model's response to a list of prompts using Hugging Face's pipeline.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): The loaded model.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "        prompts (list): A list of input prompts as strings.\n",
    "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the model's responses to the prompts.\n",
    "    \"\"\"\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    responses = [\n",
    "        text_generator(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=10, temperature=0.7)[0][\n",
    "            \"generated_text\"]\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    return responses\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Write a docstring for the following Python code:\\n [Function]\\ndef add_numbers(a, b): return a + b \\n [Docstring]\\n\",\n",
    "    \"Write a docstring for the following Python code:\\n [Function]\\ndef subtract_numbers(a, b): return a - b \\n [Docstring]\\n\",\n",
    "]\n",
    "\n",
    "responses = test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=100)\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Prompt {i + 1} - Response:\\n {response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:25:05.460034Z",
     "end_time": "2024-12-27T19:25:32.609645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.word_embeddings\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.self_attention\n",
      "transformer.h.0.self_attention.query_key_value\n",
      "transformer.h.0.self_attention.dense\n",
      "transformer.h.0.self_attention.attention_dropout\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.dense_h_to_4h\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dense_4h_to_h\n",
      "transformer.h.0.input_layernorm\n",
      "transformer.h.0.post_attention_layernorm\n",
      "transformer.h.1\n",
      "transformer.h.1.self_attention\n",
      "transformer.h.1.self_attention.query_key_value\n",
      "transformer.h.1.self_attention.dense\n",
      "transformer.h.1.self_attention.attention_dropout\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.dense_h_to_4h\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dense_4h_to_h\n",
      "transformer.h.1.input_layernorm\n",
      "transformer.h.1.post_attention_layernorm\n",
      "transformer.h.2\n",
      "transformer.h.2.self_attention\n",
      "transformer.h.2.self_attention.query_key_value\n",
      "transformer.h.2.self_attention.dense\n",
      "transformer.h.2.self_attention.attention_dropout\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.dense_h_to_4h\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dense_4h_to_h\n",
      "transformer.h.2.input_layernorm\n",
      "transformer.h.2.post_attention_layernorm\n",
      "transformer.h.3\n",
      "transformer.h.3.self_attention\n",
      "transformer.h.3.self_attention.query_key_value\n",
      "transformer.h.3.self_attention.dense\n",
      "transformer.h.3.self_attention.attention_dropout\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.dense_h_to_4h\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dense_4h_to_h\n",
      "transformer.h.3.input_layernorm\n",
      "transformer.h.3.post_attention_layernorm\n",
      "transformer.h.4\n",
      "transformer.h.4.self_attention\n",
      "transformer.h.4.self_attention.query_key_value\n",
      "transformer.h.4.self_attention.dense\n",
      "transformer.h.4.self_attention.attention_dropout\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.dense_h_to_4h\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dense_4h_to_h\n",
      "transformer.h.4.input_layernorm\n",
      "transformer.h.4.post_attention_layernorm\n",
      "transformer.h.5\n",
      "transformer.h.5.self_attention\n",
      "transformer.h.5.self_attention.query_key_value\n",
      "transformer.h.5.self_attention.dense\n",
      "transformer.h.5.self_attention.attention_dropout\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.dense_h_to_4h\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dense_4h_to_h\n",
      "transformer.h.5.input_layernorm\n",
      "transformer.h.5.post_attention_layernorm\n",
      "transformer.h.6\n",
      "transformer.h.6.self_attention\n",
      "transformer.h.6.self_attention.query_key_value\n",
      "transformer.h.6.self_attention.dense\n",
      "transformer.h.6.self_attention.attention_dropout\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.dense_h_to_4h\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dense_4h_to_h\n",
      "transformer.h.6.input_layernorm\n",
      "transformer.h.6.post_attention_layernorm\n",
      "transformer.h.7\n",
      "transformer.h.7.self_attention\n",
      "transformer.h.7.self_attention.query_key_value\n",
      "transformer.h.7.self_attention.dense\n",
      "transformer.h.7.self_attention.attention_dropout\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.dense_h_to_4h\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dense_4h_to_h\n",
      "transformer.h.7.input_layernorm\n",
      "transformer.h.7.post_attention_layernorm\n",
      "transformer.h.8\n",
      "transformer.h.8.self_attention\n",
      "transformer.h.8.self_attention.query_key_value\n",
      "transformer.h.8.self_attention.dense\n",
      "transformer.h.8.self_attention.attention_dropout\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.dense_h_to_4h\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dense_4h_to_h\n",
      "transformer.h.8.input_layernorm\n",
      "transformer.h.8.post_attention_layernorm\n",
      "transformer.h.9\n",
      "transformer.h.9.self_attention\n",
      "transformer.h.9.self_attention.query_key_value\n",
      "transformer.h.9.self_attention.dense\n",
      "transformer.h.9.self_attention.attention_dropout\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.dense_h_to_4h\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dense_4h_to_h\n",
      "transformer.h.9.input_layernorm\n",
      "transformer.h.9.post_attention_layernorm\n",
      "transformer.h.10\n",
      "transformer.h.10.self_attention\n",
      "transformer.h.10.self_attention.query_key_value\n",
      "transformer.h.10.self_attention.dense\n",
      "transformer.h.10.self_attention.attention_dropout\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.dense_h_to_4h\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dense_4h_to_h\n",
      "transformer.h.10.input_layernorm\n",
      "transformer.h.10.post_attention_layernorm\n",
      "transformer.h.11\n",
      "transformer.h.11.self_attention\n",
      "transformer.h.11.self_attention.query_key_value\n",
      "transformer.h.11.self_attention.dense\n",
      "transformer.h.11.self_attention.attention_dropout\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.dense_h_to_4h\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dense_4h_to_h\n",
      "transformer.h.11.input_layernorm\n",
      "transformer.h.11.post_attention_layernorm\n",
      "transformer.h.12\n",
      "transformer.h.12.self_attention\n",
      "transformer.h.12.self_attention.query_key_value\n",
      "transformer.h.12.self_attention.dense\n",
      "transformer.h.12.self_attention.attention_dropout\n",
      "transformer.h.12.mlp\n",
      "transformer.h.12.mlp.dense_h_to_4h\n",
      "transformer.h.12.mlp.act\n",
      "transformer.h.12.mlp.dense_4h_to_h\n",
      "transformer.h.12.input_layernorm\n",
      "transformer.h.12.post_attention_layernorm\n",
      "transformer.h.13\n",
      "transformer.h.13.self_attention\n",
      "transformer.h.13.self_attention.query_key_value\n",
      "transformer.h.13.self_attention.dense\n",
      "transformer.h.13.self_attention.attention_dropout\n",
      "transformer.h.13.mlp\n",
      "transformer.h.13.mlp.dense_h_to_4h\n",
      "transformer.h.13.mlp.act\n",
      "transformer.h.13.mlp.dense_4h_to_h\n",
      "transformer.h.13.input_layernorm\n",
      "transformer.h.13.post_attention_layernorm\n",
      "transformer.h.14\n",
      "transformer.h.14.self_attention\n",
      "transformer.h.14.self_attention.query_key_value\n",
      "transformer.h.14.self_attention.dense\n",
      "transformer.h.14.self_attention.attention_dropout\n",
      "transformer.h.14.mlp\n",
      "transformer.h.14.mlp.dense_h_to_4h\n",
      "transformer.h.14.mlp.act\n",
      "transformer.h.14.mlp.dense_4h_to_h\n",
      "transformer.h.14.input_layernorm\n",
      "transformer.h.14.post_attention_layernorm\n",
      "transformer.h.15\n",
      "transformer.h.15.self_attention\n",
      "transformer.h.15.self_attention.query_key_value\n",
      "transformer.h.15.self_attention.dense\n",
      "transformer.h.15.self_attention.attention_dropout\n",
      "transformer.h.15.mlp\n",
      "transformer.h.15.mlp.dense_h_to_4h\n",
      "transformer.h.15.mlp.act\n",
      "transformer.h.15.mlp.dense_4h_to_h\n",
      "transformer.h.15.input_layernorm\n",
      "transformer.h.15.post_attention_layernorm\n",
      "transformer.h.16\n",
      "transformer.h.16.self_attention\n",
      "transformer.h.16.self_attention.query_key_value\n",
      "transformer.h.16.self_attention.dense\n",
      "transformer.h.16.self_attention.attention_dropout\n",
      "transformer.h.16.mlp\n",
      "transformer.h.16.mlp.dense_h_to_4h\n",
      "transformer.h.16.mlp.act\n",
      "transformer.h.16.mlp.dense_4h_to_h\n",
      "transformer.h.16.input_layernorm\n",
      "transformer.h.16.post_attention_layernorm\n",
      "transformer.h.17\n",
      "transformer.h.17.self_attention\n",
      "transformer.h.17.self_attention.query_key_value\n",
      "transformer.h.17.self_attention.dense\n",
      "transformer.h.17.self_attention.attention_dropout\n",
      "transformer.h.17.mlp\n",
      "transformer.h.17.mlp.dense_h_to_4h\n",
      "transformer.h.17.mlp.act\n",
      "transformer.h.17.mlp.dense_4h_to_h\n",
      "transformer.h.17.input_layernorm\n",
      "transformer.h.17.post_attention_layernorm\n",
      "transformer.h.18\n",
      "transformer.h.18.self_attention\n",
      "transformer.h.18.self_attention.query_key_value\n",
      "transformer.h.18.self_attention.dense\n",
      "transformer.h.18.self_attention.attention_dropout\n",
      "transformer.h.18.mlp\n",
      "transformer.h.18.mlp.dense_h_to_4h\n",
      "transformer.h.18.mlp.act\n",
      "transformer.h.18.mlp.dense_4h_to_h\n",
      "transformer.h.18.input_layernorm\n",
      "transformer.h.18.post_attention_layernorm\n",
      "transformer.h.19\n",
      "transformer.h.19.self_attention\n",
      "transformer.h.19.self_attention.query_key_value\n",
      "transformer.h.19.self_attention.dense\n",
      "transformer.h.19.self_attention.attention_dropout\n",
      "transformer.h.19.mlp\n",
      "transformer.h.19.mlp.dense_h_to_4h\n",
      "transformer.h.19.mlp.act\n",
      "transformer.h.19.mlp.dense_4h_to_h\n",
      "transformer.h.19.input_layernorm\n",
      "transformer.h.19.post_attention_layernorm\n",
      "transformer.h.20\n",
      "transformer.h.20.self_attention\n",
      "transformer.h.20.self_attention.query_key_value\n",
      "transformer.h.20.self_attention.dense\n",
      "transformer.h.20.self_attention.attention_dropout\n",
      "transformer.h.20.mlp\n",
      "transformer.h.20.mlp.dense_h_to_4h\n",
      "transformer.h.20.mlp.act\n",
      "transformer.h.20.mlp.dense_4h_to_h\n",
      "transformer.h.20.input_layernorm\n",
      "transformer.h.20.post_attention_layernorm\n",
      "transformer.h.21\n",
      "transformer.h.21.self_attention\n",
      "transformer.h.21.self_attention.query_key_value\n",
      "transformer.h.21.self_attention.dense\n",
      "transformer.h.21.self_attention.attention_dropout\n",
      "transformer.h.21.mlp\n",
      "transformer.h.21.mlp.dense_h_to_4h\n",
      "transformer.h.21.mlp.act\n",
      "transformer.h.21.mlp.dense_4h_to_h\n",
      "transformer.h.21.input_layernorm\n",
      "transformer.h.21.post_attention_layernorm\n",
      "transformer.h.22\n",
      "transformer.h.22.self_attention\n",
      "transformer.h.22.self_attention.query_key_value\n",
      "transformer.h.22.self_attention.dense\n",
      "transformer.h.22.self_attention.attention_dropout\n",
      "transformer.h.22.mlp\n",
      "transformer.h.22.mlp.dense_h_to_4h\n",
      "transformer.h.22.mlp.act\n",
      "transformer.h.22.mlp.dense_4h_to_h\n",
      "transformer.h.22.input_layernorm\n",
      "transformer.h.22.post_attention_layernorm\n",
      "transformer.h.23\n",
      "transformer.h.23.self_attention\n",
      "transformer.h.23.self_attention.query_key_value\n",
      "transformer.h.23.self_attention.dense\n",
      "transformer.h.23.self_attention.attention_dropout\n",
      "transformer.h.23.mlp\n",
      "transformer.h.23.mlp.dense_h_to_4h\n",
      "transformer.h.23.mlp.act\n",
      "transformer.h.23.mlp.dense_4h_to_h\n",
      "transformer.h.23.input_layernorm\n",
      "transformer.h.23.post_attention_layernorm\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "# Show layers\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:25:42.165669Z",
     "end_time": "2024-12-27T19:25:42.170905Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ../Models/tiiuae/falcon-rw-1b_run35cd1c0e2953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/20000 : < :, Epoch 0.00/9]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 154\u001B[0m\n\u001B[1;32m    151\u001B[0m         module \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;66;03m# Log model metrics to MLflow\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mlog_history:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2165\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2169\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:2591\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2589\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[1;32m   2590\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m-> 2591\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2592\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_time\u001B[49m\n\u001B[1;32m   2593\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2594\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2595\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:3049\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001B[0m\n\u001B[1;32m   3047\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_evaluate:\n\u001B[0;32m-> 3049\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3050\u001B[0m     is_new_best_metric \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_determine_best_metric(metrics\u001B[38;5;241m=\u001B[39mmetrics, trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[1;32m   3052\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave_strategy \u001B[38;5;241m==\u001B[39m SaveStrategy\u001B[38;5;241m.\u001B[39mBEST:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:3003\u001B[0m, in \u001B[0;36mTrainer._evaluate\u001B[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001B[0m\n\u001B[1;32m   3002\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_evaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, ignore_keys_for_eval, skip_scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m-> 3003\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3004\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step, metrics)\n\u001B[1;32m   3006\u001B[0m     \u001B[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:4050\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   4047\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   4049\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 4050\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4051\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4052\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4053\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   4054\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   4055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   4056\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4057\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4058\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4060\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   4061\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:4244\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   4241\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m observed_batch_size\n\u001B[1;32m   4243\u001B[0m \u001B[38;5;66;03m# Prediction step\u001B[39;00m\n\u001B[0;32m-> 4244\u001B[0m losses, logits, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4245\u001B[0m main_input_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain_input_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   4246\u001B[0m inputs_decode \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   4247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs[main_input_name]) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   4248\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:4460\u001B[0m, in \u001B[0;36mTrainer.prediction_step\u001B[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001B[0m\n\u001B[1;32m   4458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_labels \u001B[38;5;129;01mor\u001B[39;00m loss_without_labels:\n\u001B[1;32m   4459\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 4460\u001B[0m         loss, outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   4461\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m   4463\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:3708\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3706\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3707\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3708\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3709\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3710\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/peft_model.py:1719\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1718\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1724\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1725\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1730\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-rw-1b/e4b9872bb803165eb22f0a867d4e6a64d34fce19/modeling_falcon.py:900\u001B[0m, in \u001B[0;36mFalconForCausalLM.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    891\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m    896\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    898\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 900\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    903\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    905\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    906\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    907\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    908\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    909\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    910\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    911\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    913\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-rw-1b/e4b9872bb803165eb22f0a867d4e6a64d34fce19/modeling_falcon.py:797\u001B[0m, in \u001B[0;36mFalconModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    789\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    790\u001B[0m         create_custom_forward(block),\n\u001B[1;32m    791\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    794\u001B[0m         head_mask[i],\n\u001B[1;32m    795\u001B[0m     )\n\u001B[1;32m    796\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 797\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[43m        \u001B[49m\u001B[43malibi\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malibi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    807\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-rw-1b/e4b9872bb803165eb22f0a867d4e6a64d34fce19/modeling_falcon.py:453\u001B[0m, in \u001B[0;36mFalconDecoderLayer.forward\u001B[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    450\u001B[0m     attention_layernorm_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    452\u001B[0m \u001B[38;5;66;03m# Self attention.\u001B[39;00m\n\u001B[0;32m--> 453\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_layernorm_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43malibi\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malibi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    463\u001B[0m attention_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnew_decoder_architecture:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-rw-1b/e4b9872bb803165eb22f0a867d4e6a64d34fce19/modeling_falcon.py:390\u001B[0m, in \u001B[0;36mFalconAttention.forward\u001B[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;66;03m# change view [batch_size, num_heads, q_length, head_dim]\u001B[39;00m\n\u001B[1;32m    388\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_heads(context_layer)\n\u001B[0;32m--> 390\u001B[0m output_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output_tensor, present, attention_probs\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:609\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[1;32m    607\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_layer(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 609\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    610\u001B[0m     torch_result_dtype \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    611\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m active_adapter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapters:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-rw-1b/e4b9872bb803165eb22f0a867d4e6a64d34fce19/modeling_falcon.py:57\u001B[0m, in \u001B[0;36mFalconLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m     56\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m@\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1675\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1666\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[1;32m   1668\u001B[0m \u001B[38;5;66;03m# On the return type:\u001B[39;00m\n\u001B[1;32m   1669\u001B[0m \u001B[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001B[39;00m\n\u001B[1;32m   1670\u001B[0m \u001B[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1673\u001B[0m \u001B[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001B[39;00m\n\u001B[1;32m   1674\u001B[0m \u001B[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001B[39;00m\n\u001B[0;32m-> 1675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m   1676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1677\u001B[0m         _parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tuning configuration\n",
    "model_name = model_to_finetune\n",
    "\n",
    "# Generate a random run ID\n",
    "current_time = str(time.time()).encode('utf-8')\n",
    "hash_object = hashlib.sha256(current_time)\n",
    "hex_digest = hash_object.hexdigest()\n",
    "random_string = hex_digest[:12]\n",
    "run_id = random_string\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.3\n",
    "\n",
    "# Training parameter\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 1.5e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"adamw_torch\"\n",
    "lr_scheduler_type = \"constant_with_warmup\"\n",
    "max_steps = 20000\n",
    "warmup_ratio = 0.01\n",
    "group_by_length = True\n",
    "save_steps = 100\n",
    "logging_steps = 1\n",
    "eval_steps = 2\n",
    "\n",
    "# Output directory\n",
    "run_name = f\"{model_name}_run{run_id}\"\n",
    "output_dir = os.path.join(base_output_dir, run_name)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = os.path.join(output_dir, \"end_of_training\")\n",
    "\n",
    "# Target modules to adapt key components to the model type (Falcon / CodeLlama):\n",
    "# - Attention Projections: Query, Key, Value, and Output\n",
    "# - Feed-Forward Network: Input (Expansion) and Output (Reduction)\n",
    "# - Embedding Matrix: Maps tokens to dense vectors\n",
    "if \"CodeLlama\" in model_name:\n",
    "    target_modules = [\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "        \"embed_tokens\",\n",
    "    ]\n",
    "elif \"falcon\" in model_name:\n",
    "    target_modules = [\n",
    "        \"self_attention.query_key_value\",\n",
    "        \"self_attention.dense\",\n",
    "        \"mlp.dense_h_to_4h\",\n",
    "        \"mlp.dense_4h_to_h\",\n",
    "        \"word_embeddings\",\n",
    "    ]\n",
    "else:\n",
    "    target_modules = None\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"all\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "\n",
    "# Monitoring\n",
    "projectname='DocstringGenerator'\n",
    "\n",
    "# Initialize MLflow\n",
    "if on_colab:\n",
    "    # Starts MLflow UI in the background\n",
    "    get_ipython().system_raw(\"mlflow ui --backend-store-uri file:/content/mlruns --port 5000 &\")\n",
    "    # Forward port 5000 via ngrok\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(\"MLflow Tracking UI:\", public_url.public_url)\n",
    "    mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
    "else:\n",
    "    # run in terminal:\n",
    "    # mlflow server --host 127.0.0.1 --port 8080\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(projectname)\n",
    "mlflow.start_run(run_name=f\"run_{run_id}\")\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.login(key=wandb_api_token)\n",
    "wandb.init(\n",
    "    project=projectname,\n",
    "    name=f\"run_{run_id}\",\n",
    "    config={\n",
    "    \"lora_r\":lora_r,\n",
    "    \"lora_dropout\":lora_dropout,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"num_train_epochs\": num_train_epochs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    report_to=[\"wandb\", \"mlflow\"],\n",
    "    run_name=run_id,\n",
    "    logging_dir=os.path.join(base_output_dir, \"Results/runs/\", run_name),\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(),\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # max_seq_length=None,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    # packing=False,\n",
    ")\n",
    "\n",
    "# Pre-process the model of layer norm for stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Log model metrics to MLflow\n",
    "if trainer.state.log_history:\n",
    "    metrics = trainer.state.log_history[-1]\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "# Save the trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "mlflow.log_artifacts(output_dir)\n",
    "\n",
    "# End MLflow and W&B session\n",
    "mlflow.end_run()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers.utils.hub import TRANSFORMERS_CACHE\n",
    "\n",
    "print(\"Hugging Face Cache Directory:\", TRANSFORMERS_CACHE)\n",
    "\n",
    "# du -sh *\n",
    "# rm -rf *\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-20T14:36:12.893443Z",
     "end_time": "2024-12-20T14:36:12.901135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run run_e37c93546093 at: http://127.0.0.1:8080/#/experiments/279081502891501101/runs/09dd9645079f40098dda222cbd0b441c\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/279081502891501101\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-27T19:41:42.555351Z",
     "end_time": "2024-12-27T19:41:43.170866Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
