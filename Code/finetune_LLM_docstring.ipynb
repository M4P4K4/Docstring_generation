{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:27.070195Z",
     "end_time": "2025-01-07T12:39:30.556523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: peft in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: transformers in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (4.47.1)\r\n",
      "Requirement already satisfied: trl in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: datasets in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: wandb in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (0.19.1)\r\n",
      "Requirement already satisfied: mlflow in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (2.19.0)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: pyngrok in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (7.2.2)\r\n",
      "Requirement already satisfied: numpy==1.24.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (24.1)\r\n",
      "Requirement already satisfied: psutil in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (0.27.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from accelerate) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from peft) (4.67.1)\r\n",
      "Requirement already satisfied: scipy in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from bitsandbytes) (1.14.1)\r\n",
      "Requirement already satisfied: filelock in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (3.16.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: rich in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from trl) (13.9.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: xxhash in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from datasets) (3.11.10)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (4.3.6)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (5.29.1)\r\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (2.10.4)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (2.19.2)\r\n",
      "Requirement already satisfied: setproctitle in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (1.3.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (67.7.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: mlflow-skinny==2.19.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (2.19.0)\r\n",
      "Requirement already satisfied: Flask<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.1.4)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (1.14.0)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (7.1.0)\r\n",
      "Requirement already satisfied: graphene<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.4.3)\r\n",
      "Requirement already satisfied: gunicorn<24 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (23.0.0)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.7)\r\n",
      "Requirement already satisfied: matplotlib<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (3.9.2)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (1.5.2)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow) (2.0.36)\r\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.39.0)\r\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\r\n",
      "Requirement already satisfied: Mako in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Flask<4->mlflow) (1.9.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.5)\r\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from graphene<4->mlflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.54.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.7)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.5.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from rich->trl) (2.18.0)\r\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/mariusbernahrndt/Desktop/CryptoCounter/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Configuring the character encoding\n",
    "import locale\n",
    "\n",
    "\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "!pip install -U accelerate peft bitsandbytes transformers trl datasets wandb mlflow python-dotenv pyngrok numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import numpy\n",
    "import random\n",
    "import mlflow\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Features, Value\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from pyngrok import ngrok"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:32.066090Z",
     "end_time": "2025-01-07T12:39:32.081611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:35.192584Z",
     "end_time": "2025-01-07T12:39:35.347422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is running locally.\n",
      "Access token loaded.\n"
     ]
    }
   ],
   "source": [
    "# Check if the notebook runs on Colab to adjust paths\n",
    "on_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if on_colab:\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    test_file = \"/content/drive/MyDrive/Data/docstring_test_data.txt\"\n",
    "    train_file = \"/content/drive/MyDrive/Data/docstring_training_data.txt\"\n",
    "    base_output_dir = \"/content/drive/MyDrive/Models/\"\n",
    "    print(\"The notebook runs on Google Colab.\")\n",
    "\n",
    "    # Load API-Token from Colab-Secrets\n",
    "    huggingface_api_token = userdata.get('huggingface_api_token')\n",
    "    wandb_api_token = userdata.get('wandb_api_token')\n",
    "else:\n",
    "    test_file = \"../Data/docstring_test_data.txt\"\n",
    "    train_file = \"../Data/docstring_training_data.txt\"\n",
    "    base_output_dir = \"../Models/\"\n",
    "    print(\"The notebook is running locally.\")\n",
    "\n",
    "    # Load API-Token from .env\n",
    "    load_dotenv()\n",
    "    huggingface_api_token = os.getenv(\"HUGGINGFACE\")\n",
    "    wandb_api_token = os.getenv(\"WANDB\")\n",
    "\n",
    "if huggingface_api_token and wandb_api_token:\n",
    "    print(\"Access token loaded.\")\n",
    "else:\n",
    "    print(\"Access token not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:37.623651Z",
     "end_time": "2025-01-07T12:39:37.641151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def load_dataset_from_text_files(train_file_path, test_file_path):\n",
    "    \"\"\"\n",
    "    Load training and test datasets from text files into a DatasetDict.\n",
    "\n",
    "    Args:\n",
    "        train_file_path (str): Path to the training data text file.\n",
    "        test_file_path (str): Path to the test data text file.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A dictionary containing 'train' and 'test' datasets with text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_lines_from_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        print(f'Total lines loaded from {file_path}: {len(lines)}')\n",
    "        return lines\n",
    "\n",
    "    def get_dataset_generator(file_path):\n",
    "        lines = get_lines_from_file(file_path)\n",
    "        for line in lines:\n",
    "            yield {\"text\": line}\n",
    "\n",
    "    dataset_train = Dataset.from_generator(\n",
    "        generator=lambda: get_dataset_generator(train_file_path),\n",
    "        features=Features({'text': Value('string')})\n",
    "    )\n",
    "    dataset_test = Dataset.from_generator(\n",
    "        generator=lambda: get_dataset_generator(test_file_path),\n",
    "        features=Features({'text': Value('string')})\n",
    "    )\n",
    "    return DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
    "\n",
    "\n",
    "datasets = load_dataset_from_text_files(train_file, test_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:43.350295Z",
     "end_time": "2025-01-07T12:39:43.440839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is running locally: Using bfloat16 precision.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99c5d0193aef429986bde485e7a582e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model selection and configuration\n",
    "model_to_finetune = \"meta-llama/CodeLlama-7b-Python-hf\"\n",
    "# model_to_finetune = \"tiiuae/falcon-rw-1b\"\n",
    "# model_to_finetune = \"tiiuae/falcon-7b\"\n",
    "\n",
    "if on_colab:\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    torch_dtype = None\n",
    "    device_map = \"auto\"\n",
    "    print(\"Notebook is running on Colab: Using 4-bit quantization.\")\n",
    "else:\n",
    "    quantization_config = None\n",
    "    torch_dtype = torch.bfloat16\n",
    "    device_map = \"cpu\"\n",
    "    print(\"Notebook is running locally: Using bfloat16 precision.\")\n",
    "\n",
    "# Load tokeniser and model with auth token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_to_finetune,\n",
    "    token=huggingface_api_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_to_finetune,\n",
    "    token=huggingface_api_token,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Deactivating cache & setting pretraining\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:45.346386Z",
     "end_time": "2025-01-07T12:39:48.421172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example 1824 : {'text': '[Function] def batch_random_flip(input_):\\\\n    if isinstance(input_, (float, int)):\\\\n        return input_\\\\n    shape = input_.get_shape().as_list()\\\\n    batch_size = shape[0]\\\\n    height = shape[1]\\\\n    width = shape[2]\\\\n    channels = shape[3]\\\\n    res = tf.split(axis=0, num_or_size_splits=batch_size, value=input_)\\\\n    res = [elem[0, :, :, :] for elem in res]\\\\n    res = [tf.image.random_flip_left_right(elem) for elem in res]\\\\n    res = [tf.reshape(elem, [1, height, width, channels]) for elem in res]\\\\n    res = tf.concat(axis=0, values=res)\\\\n    return res [Docstring] Simultaneous horizontal random flip.', 'input_ids': [1, 518, 6678, 29962, 822, 9853, 29918, 8172, 29918, 29888, 3466, 29898, 2080, 29918, 1125, 29905, 29876, 1678, 565, 338, 8758, 29898, 2080, 3383, 313, 7411, 29892, 938, 876, 3583, 29876, 4706, 736, 1881, 3187, 29876, 1678, 8267, 353, 1881, 5396, 657, 29918, 12181, 2141, 294, 29918, 1761, 580, 29905, 29876, 1678, 9853, 29918, 2311, 353, 8267, 29961, 29900, 10725, 29876, 1678, 3171, 353, 8267, 29961, 29896, 10725, 29876, 1678, 2920, 353, 8267, 29961, 29906, 10725, 29876, 1678, 18196, 353, 8267, 29961, 29941, 10725, 29876, 1678, 620, 353, 15886, 29889, 5451, 29898, 8990, 29922, 29900, 29892, 954, 29918, 272, 29918, 2311, 29918, 23579, 1169, 29922, 16175, 29918, 2311, 29892, 995, 29922, 2080, 29918, 2144, 29876, 1678, 620, 353, 518, 20461, 29961, 29900, 29892, 584, 29892, 584, 29892, 584, 29962, 363, 21268, 297, 620, 10725, 29876, 1678, 620, 353, 518, 13264, 29889, 3027, 29889, 8172, 29918, 29888, 3466, 29918, 1563, 29918, 1266, 29898, 20461, 29897, 363, 21268, 297, 620, 10725, 29876, 1678, 620, 353, 518, 13264, 29889, 690, 14443, 29898, 20461, 29892, 518, 29896, 29892, 3171, 29892, 2920, 29892, 18196, 2314, 363, 21268, 297, 620, 10725, 29876, 1678, 620, 353, 15886, 29889, 17685, 29898, 8990, 29922, 29900, 29892, 1819, 29922, 690, 2144, 29876, 1678, 736, 620, 518, 14526, 1807, 29962, 3439, 499, 23584, 14698, 4036, 285, 3466, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 409 : {'text': '[Function] def upsample(img, is_flow):\\\\n    (_, height, width, _) = img.shape.as_list()\\\\n    orig_dtype = img.dtype\\\\n    if orig_dtype != tf.float32:\\\\n        img = tf.cast(img, tf.float32)\\\\n    img_resized = tf.compat.v2.image.resize(img, (int(height * 2), int(width * 2)))\\\\n    if is_flow:\\\\n        img_resized *= 2\\\\n    if img_resized.dtype != orig_dtype:\\\\n        return tf.cast(img_resized, orig_dtype)\\\\n    return img_resized [Docstring] Double resolution of an image or flow field.\\\\n\\\\nArgs:\\\\n  img: tf.tensor, image or flow field to be resized\\\\n  is_flow: bool, flag for scaling flow accordingly\\\\n\\\\nReturns:\\\\n  Resized and potentially scaled image or flow field.', 'input_ids': [1, 518, 6678, 29962, 822, 24081, 981, 29898, 2492, 29892, 338, 29918, 1731, 1125, 29905, 29876, 1678, 313, 3383, 3171, 29892, 2920, 29892, 24459, 353, 10153, 29889, 12181, 29889, 294, 29918, 1761, 580, 29905, 29876, 1678, 1677, 29918, 29881, 1853, 353, 10153, 29889, 29881, 1853, 29905, 29876, 1678, 565, 1677, 29918, 29881, 1853, 2804, 15886, 29889, 7411, 29941, 29906, 3583, 29876, 4706, 10153, 353, 15886, 29889, 4384, 29898, 2492, 29892, 15886, 29889, 7411, 29941, 29906, 2144, 29876, 1678, 10153, 29918, 690, 1891, 353, 15886, 29889, 12667, 29889, 29894, 29906, 29889, 3027, 29889, 21476, 29898, 2492, 29892, 313, 524, 29898, 3545, 334, 29871, 29906, 511, 938, 29898, 2103, 334, 29871, 29906, 876, 2144, 29876, 1678, 565, 338, 29918, 1731, 3583, 29876, 4706, 10153, 29918, 690, 1891, 334, 29922, 29871, 29906, 29905, 29876, 1678, 565, 10153, 29918, 690, 1891, 29889, 29881, 1853, 2804, 1677, 29918, 29881, 1853, 3583, 29876, 4706, 736, 15886, 29889, 4384, 29898, 2492, 29918, 690, 1891, 29892, 1677, 29918, 29881, 1853, 2144, 29876, 1678, 736, 10153, 29918, 690, 1891, 518, 14526, 1807, 29962, 11599, 10104, 310, 385, 1967, 470, 4972, 1746, 7790, 29876, 29905, 29876, 7883, 3583, 29876, 29871, 10153, 29901, 15886, 29889, 20158, 29892, 1967, 470, 4972, 1746, 304, 367, 620, 1891, 29905, 29876, 29871, 338, 29918, 1731, 29901, 6120, 29892, 7353, 363, 21640, 4972, 16205, 29905, 29876, 29905, 29876, 11609, 29879, 3583, 29876, 29871, 2538, 1891, 322, 19998, 6287, 29881, 1967, 470, 4972, 1746, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 4506 : {'text': \"[Function] def extract_patches(images, patch_shape, samples_per_image=40, seed=0, cycle=True):\\\\n    rs = np.random.RandomState(seed)\\\\n    for Xi in itr.cycle(images):\\\\n        (w, h) = [Xi.shape[i] - patch_shape[i] for i in range(2)]\\\\n        assert w > 0 and h > 0\\\\n        indices = np.asarray(list(itr.product(range(w), range(h))))\\\\n        rs.shuffle(indices)\\\\n        for (x, y) in indices[:samples_per_image]:\\\\n            yield Xi[x:x + patch_shape[0], y:y + patch_shape[1]] [Docstring] Takes a set of images and yields randomly chosen patches of specified size.\\\\n\\\\nParameters\\\\n----------\\\\nimages : iterable\\\\n    The images have to be iterable, and each element must be a Numpy array\\\\n    with at least two spatial 2 dimensions as the first and second axis.\\\\npatch_shape : tuple, length 2\\\\n    The spatial shape of the patches that should be extracted. If the\\\\n    images have further dimensions beyond the spatial, the patches will\\\\n    copy these too.\\\\nsamples_per_image : int\\\\n    Samples to extract before moving on to the next image.\\\\nseed : int\\\\n    Seed with which to select the patches.\\\\ncycle : bool\\\\n    If True, then the function will produce patches indefinitely, by going\\\\n    back to the first image when all are done. If False, the iteration will\\\\n    stop when there are no more images.\\\\n\\\\nReturns\\\\n-------\\\\npatch_generator\\\\n    This function returns a generator that will produce patches.\\\\n\\\\nExamples\\\\n--------\\\\n>>> import deepdish as dd\\\\n>>> import matplotlib.pylab as plt\\\\n>>> import itertools\\\\n>>> images = ag.io.load_example('mnist')\\\\n\\\\nNow, let us say we want to exact patches from the these, where each patch\\\\nhas at least some activity.\\\\n\\\\n>>> gen = dd.image.extract_patches(images, (5, 5))\\\\n>>> gen = (x for x in gen if x.mean() > 0.1)\\\\n>>> patches = np.array(list(itertools.islice(gen, 25)))\\\\n>>> patches.shape\\\\n(25, 5, 5)\\\\n>>> dd.plot.images(patches)\\\\n>>> plt.show()\", 'input_ids': [1, 518, 6678, 29962, 822, 6597, 29918, 5041, 267, 29898, 8346, 29892, 13261, 29918, 12181, 29892, 11916, 29918, 546, 29918, 3027, 29922, 29946, 29900, 29892, 16717, 29922, 29900, 29892, 11412, 29922, 5574, 1125, 29905, 29876, 1678, 20371, 353, 7442, 29889, 8172, 29889, 17875, 2792, 29898, 26776, 2144, 29876, 1678, 363, 1060, 29875, 297, 372, 29878, 29889, 23090, 29898, 8346, 1125, 29905, 29876, 4706, 313, 29893, 29892, 298, 29897, 353, 518, 29990, 29875, 29889, 12181, 29961, 29875, 29962, 448, 13261, 29918, 12181, 29961, 29875, 29962, 363, 474, 297, 3464, 29898, 29906, 4638, 29905, 29876, 4706, 4974, 281, 1405, 29871, 29900, 322, 298, 1405, 29871, 29900, 29905, 29876, 4706, 16285, 353, 7442, 29889, 294, 2378, 29898, 1761, 29898, 277, 29878, 29889, 4704, 29898, 3881, 29898, 29893, 511, 3464, 29898, 29882, 13697, 29905, 29876, 4706, 20371, 29889, 845, 21897, 29898, 513, 1575, 2144, 29876, 4706, 363, 313, 29916, 29892, 343, 29897, 297, 16285, 7503, 27736, 29918, 546, 29918, 3027, 29962, 3583, 29876, 9651, 7709, 1060, 29875, 29961, 29916, 29901, 29916, 718, 13261, 29918, 12181, 29961, 29900, 1402, 343, 29901, 29891, 718, 13261, 29918, 12181, 29961, 29896, 5262, 518, 14526, 1807, 29962, 323, 6926, 263, 731, 310, 4558, 322, 17498, 20459, 10434, 13261, 267, 310, 6790, 2159, 7790, 29876, 29905, 29876, 11507, 29905, 29876, 28400, 29905, 29876, 8346, 584, 4256, 519, 29905, 29876, 1678, 450, 4558, 505, 304, 367, 4256, 519, 29892, 322, 1269, 1543, 1818, 367, 263, 11848, 2272, 1409, 29905, 29876, 1678, 411, 472, 3203, 1023, 18652, 29871, 29906, 13391, 408, 278, 937, 322, 1473, 9685, 7790, 29876, 5041, 29918, 12181, 584, 18761, 29892, 3309, 29871, 29906, 29905, 29876, 1678, 450, 18652, 8267, 310, 278, 13261, 267, 393, 881, 367, 23892, 29889, 960, 278, 29905, 29876, 1678, 4558, 505, 4340, 13391, 8724, 278, 18652, 29892, 278, 13261, 267, 674, 29905, 29876, 1678, 3509, 1438, 2086, 7790, 1983, 9422, 29918, 546, 29918, 3027, 584, 938, 29905, 29876, 1678, 3685, 2701, 304, 6597, 1434, 8401, 373, 304, 278, 2446, 1967, 7790, 29876, 26776, 584, 938, 29905, 29876, 1678, 922, 287, 411, 607, 304, 1831, 278, 13261, 267, 7790, 29876, 23090, 584, 6120, 29905, 29876, 1678, 960, 5852, 29892, 769, 278, 740, 674, 7738, 13261, 267, 297, 1753, 18639, 29892, 491, 2675, 29905, 29876, 1678, 1250, 304, 278, 937, 1967, 746, 599, 526, 2309, 29889, 960, 7700, 29892, 278, 12541, 674, 29905, 29876, 1678, 5040, 746, 727, 526, 694, 901, 4558, 7790, 29876, 29905, 29876, 11609, 29879, 29905, 29876, 22158, 2612, 29876, 5041, 29918, 27959, 29905, 29876, 1678, 910, 740, 3639, 263, 15299, 393, 674, 7738, 13261, 267, 7790, 29876, 29905, 29876, 1252, 9422, 29905, 29876, 1378, 29905, 29876, 6778, 29958, 1053, 6483, 29881, 728, 408, 24488, 29905, 29876, 6778, 29958, 1053, 22889, 29889, 2272, 8205, 408, 14770, 29905, 29876, 6778, 29958, 1053, 4256, 8504, 29905, 29876, 6778, 29958, 4558, 353, 946, 29889, 601, 29889, 1359, 29918, 4773, 877, 23521, 391, 1495, 29905, 29876, 29905, 29876, 10454, 29892, 1235, 502, 1827, 591, 864, 304, 2684, 13261, 267, 515, 278, 1438, 29892, 988, 1269, 13261, 29905, 29876, 5349, 472, 3203, 777, 6354, 7790, 29876, 29905, 29876, 6778, 29958, 2531, 353, 24488, 29889, 3027, 29889, 21111, 29918, 5041, 267, 29898, 8346, 29892, 313, 29945, 29892, 29871, 29945, 28986, 29876, 6778, 29958, 2531, 353, 313, 29916, 363, 921, 297, 2531, 565, 921, 29889, 12676, 580, 1405, 29871, 29900, 29889, 29896, 2144, 29876, 6778, 29958, 13261, 267, 353, 7442, 29889, 2378, 29898, 1761, 29898, 1524, 8504, 29889, 275, 5897, 29898, 1885, 29892, 29871, 29906, 29945, 876, 2144, 29876, 6778, 29958, 13261, 267, 29889, 12181, 29905, 29876, 29898, 29906, 29945, 29892, 29871, 29945, 29892, 29871, 29945, 2144, 29876, 6778, 29958, 24488, 29889, 5317, 29889, 8346, 29898, 5041, 267, 2144, 29876, 6778, 29958, 14770, 29889, 4294, 580], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 4012 : {'text': '[Function] def random_ranking(input_image, global_mean, threshold, keep_information):\\\\n    total_pixels = IMAGE_DIMS[0] * IMAGE_DIMS[1] * IMAGE_DIMS[2]\\\\n    input_image = tf.reshape(input_image, [total_pixels])\\\\n    global_mean_constant = tf.constant(global_mean, shape=[1, 1, 3])\\\\n    substitute_information = tf.reshape(tf.multiply(global_mean_constant, 1.0), [total_pixels])\\\\n    fraction = threshold / 100.0\\\\n    multiple = tf.constant(1.0, shape=[total_pixels], dtype=tf.float32)\\\\n    drop_out = tf.nn.dropout(multiple, keep_prob=fraction)\\\\n    boolean_mask = tf.cast(drop_out, tf.bool)\\\\n    if keep_information:\\\\n        feature_ranking = tf.where(boolean_mask, input_image, substitute_information)\\\\n    else:\\\\n        feature_ranking = tf.where(boolean_mask, substitute_information, input_image)\\\\n    feature_ranking = tf.reshape(feature_ranking, IMAGE_DIMS)\\\\n    return feature_ranking [Docstring] Returns an image where the pixels are randomly modified.\\\\n\\\\nDropout is applied to determine which pixels to preserve and which to\\\\nreplace with the value determined by the replace_information parameter.\\\\n\\\\nArgs:\\\\n  input_image: A 3D float tensor containing the model inputs.\\\\n  global_mean: The global mean for each color channel.\\\\n  threshold: Percentile of information in the saliency method ranking to\\\\n    retain.\\\\n  keep_information: A boolean variable that determines whether information is\\\\n    kept or is removed.\\\\n\\\\nReturns:\\\\n  feature_ranking: feature ranking input\\\\nRaises:\\\\n  ValueError: if the replacement type is not passed or does not match.', 'input_ids': [1, 518, 6678, 29962, 822, 4036, 29918, 661, 9292, 29898, 2080, 29918, 3027, 29892, 5534, 29918, 12676, 29892, 16897, 29892, 3013, 29918, 19678, 1125, 29905, 29876, 1678, 3001, 29918, 29886, 861, 1379, 353, 306, 1529, 1692, 29918, 4571, 4345, 29961, 29900, 29962, 334, 306, 1529, 1692, 29918, 4571, 4345, 29961, 29896, 29962, 334, 306, 1529, 1692, 29918, 4571, 4345, 29961, 29906, 10725, 29876, 1678, 1881, 29918, 3027, 353, 15886, 29889, 690, 14443, 29898, 2080, 29918, 3027, 29892, 518, 7827, 29918, 29886, 861, 1379, 29962, 2144, 29876, 1678, 5534, 29918, 12676, 29918, 23362, 353, 15886, 29889, 23362, 29898, 10945, 29918, 12676, 29892, 8267, 11759, 29896, 29892, 29871, 29896, 29892, 29871, 29941, 29962, 2144, 29876, 1678, 23764, 29918, 19678, 353, 15886, 29889, 690, 14443, 29898, 13264, 29889, 18056, 368, 29898, 10945, 29918, 12676, 29918, 23362, 29892, 29871, 29896, 29889, 29900, 511, 518, 7827, 29918, 29886, 861, 1379, 29962, 2144, 29876, 1678, 15958, 353, 16897, 847, 29871, 29896, 29900, 29900, 29889, 29900, 29905, 29876, 1678, 2999, 353, 15886, 29889, 23362, 29898, 29896, 29889, 29900, 29892, 8267, 11759, 7827, 29918, 29886, 861, 1379, 1402, 26688, 29922, 13264, 29889, 7411, 29941, 29906, 2144, 29876, 1678, 5768, 29918, 449, 353, 15886, 29889, 15755, 29889, 8865, 449, 29898, 20787, 29892, 3013, 29918, 22795, 29922, 29888, 13857, 2144, 29876, 1678, 7223, 29918, 13168, 353, 15886, 29889, 4384, 29898, 8865, 29918, 449, 29892, 15886, 29889, 11227, 2144, 29876, 1678, 565, 3013, 29918, 19678, 3583, 29876, 4706, 4682, 29918, 661, 9292, 353, 15886, 29889, 3062, 29898, 20054, 29918, 13168, 29892, 1881, 29918, 3027, 29892, 23764, 29918, 19678, 2144, 29876, 1678, 1683, 3583, 29876, 4706, 4682, 29918, 661, 9292, 353, 15886, 29889, 3062, 29898, 20054, 29918, 13168, 29892, 23764, 29918, 19678, 29892, 1881, 29918, 3027, 2144, 29876, 1678, 4682, 29918, 661, 9292, 353, 15886, 29889, 690, 14443, 29898, 14394, 29918, 661, 9292, 29892, 306, 1529, 1692, 29918, 4571, 4345, 2144, 29876, 1678, 736, 4682, 29918, 661, 9292, 518, 14526, 1807, 29962, 16969, 385, 1967, 988, 278, 17036, 526, 20459, 9120, 7790, 29876, 29905, 29876, 15063, 449, 338, 7436, 304, 8161, 607, 17036, 304, 19905, 322, 607, 304, 29905, 29876, 6506, 411, 278, 995, 10087, 491, 278, 5191, 29918, 19678, 3443, 7790, 29876, 29905, 29876, 7883, 3583, 29876, 29871, 1881, 29918, 3027, 29901, 319, 29871, 29941, 29928, 5785, 12489, 6943, 278, 1904, 10970, 7790, 29876, 29871, 5534, 29918, 12676, 29901, 450, 5534, 2099, 363, 1269, 2927, 8242, 7790, 29876, 29871, 16897, 29901, 2431, 1760, 488, 310, 2472, 297, 278, 4497, 13396, 1158, 24034, 304, 29905, 29876, 1678, 11551, 7790, 29876, 29871, 3013, 29918, 19678, 29901, 319, 7223, 2286, 393, 3683, 1475, 3692, 2472, 338, 29905, 29876, 1678, 8126, 470, 338, 6206, 7790, 29876, 29905, 29876, 11609, 29879, 3583, 29876, 29871, 4682, 29918, 661, 9292, 29901, 4682, 24034, 1881, 29905, 29876, 29934, 1759, 267, 3583, 29876, 29871, 7865, 2392, 29901, 565, 278, 16920, 1134, 338, 451, 4502, 470, 947, 451, 1993, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Train example 3657 : {'text': '[Function] def _gen(data):\\\\n    index_array = np.arange(num_samples)\\\\n    for _ in range(epochs):\\\\n        if shuffle:\\\\n            np.random.shuffle(index_array)\\\\n        batches = generic_utils.make_batches(num_samples, batch_size)\\\\n        for (batch_start, batch_end) in batches:\\\\n            batch_ids = index_array[batch_start:batch_end]\\\\n            batch_x = data[0][batch_ids]\\\\n            batch_missing = data[1][batch_ids]\\\\n            yield ([batch_x, batch_missing], None) [Docstring] Makes a generator out of Numpy arrays', 'input_ids': [1, 518, 6678, 29962, 822, 903, 1885, 29898, 1272, 1125, 29905, 29876, 1678, 2380, 29918, 2378, 353, 7442, 29889, 279, 927, 29898, 1949, 29918, 27736, 2144, 29876, 1678, 363, 903, 297, 3464, 29898, 1022, 2878, 29879, 1125, 29905, 29876, 4706, 565, 528, 21897, 3583, 29876, 9651, 7442, 29889, 8172, 29889, 845, 21897, 29898, 2248, 29918, 2378, 2144, 29876, 4706, 9853, 267, 353, 10035, 29918, 13239, 29889, 5675, 29918, 16175, 267, 29898, 1949, 29918, 27736, 29892, 9853, 29918, 2311, 2144, 29876, 4706, 363, 313, 16175, 29918, 2962, 29892, 9853, 29918, 355, 29897, 297, 9853, 267, 3583, 29876, 9651, 9853, 29918, 4841, 353, 2380, 29918, 2378, 29961, 16175, 29918, 2962, 29901, 16175, 29918, 355, 10725, 29876, 9651, 9853, 29918, 29916, 353, 848, 29961, 29900, 3816, 16175, 29918, 4841, 10725, 29876, 9651, 9853, 29918, 27259, 353, 848, 29961, 29896, 3816, 16175, 29918, 4841, 10725, 29876, 9651, 7709, 9310, 16175, 29918, 29916, 29892, 9853, 29918, 27259, 1402, 6213, 29897, 518, 14526, 1807, 29962, 341, 6926, 263, 15299, 714, 310, 11848, 2272, 7049], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation of the data sets for training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Example output of some training examples\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(tokenized_datasets[\"train\"]) - 1)\n",
    "    print(\"Train example\", index, \":\", tokenized_datasets[\"train\"][index])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:50.375101Z",
     "end_time": "2025-01-07T12:39:50.450731Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 - Response:\n",
      " Write a docstring for the following Python code:\n",
      " [Function]\n",
      "def add_numbers(a, b): return a + b \n",
      " [Docstring]\n",
      "\"\"\"\n",
      "\n",
      "[Function]\n",
      "def add_numers(a, b): return a + b \n",
      "[Docstring]\n",
      "\"\"\"\n",
      "\n",
      "Write a docstring for the following Python code:\n",
      "\n",
      "def add_numbers(a, b):\n",
      "    # This function adds two numbers\n",
      "    return a + b\n",
      "\n",
      "[Function]\n",
      "def add_numbers(a, b):\n",
      "    # This function adds two numbers\n",
      "    return a + b\n",
      "[Doc\n",
      "Prompt 2 - Response:\n",
      " Write a docstring for the following Python code:\n",
      " [Function]\n",
      "def subtract_numbers(a, b): return a - b \n",
      " [Docstring]\n",
      "def subtract_numbers(a, b):\n",
      "    \"\"\"\n",
      "    This function subtracts two numbers and returns the difference.\n",
      "    \n",
      "    Arguments:\n",
      "        a: An integer or float. This is the first number to subtract.\n",
      "        b: An integer or float. This is the second number to subtract.\n",
      "    \n",
      "    Returns:\n",
      "        An integer or float. This is the difference between a and b.\n",
      "    \n",
      "    \"\"\"\n",
      "    return a - b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Tests the model's response to a list of prompts using Hugging Face's pipeline.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): The loaded model.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "        prompts (list): A list of input prompts as strings.\n",
    "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the model's responses to the prompts.\n",
    "    \"\"\"\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    responses = [\n",
    "        text_generator(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=10, temperature=0.7)[0][\n",
    "            \"generated_text\"]\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    return responses\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Write a docstring for the following Python code:\\n [Function]\\ndef add_numbers(a, b): return a + b \\n [Docstring]\\n\",\n",
    "    \"Write a docstring for the following Python code:\\n [Function]\\ndef subtract_numbers(a, b): return a - b \\n [Docstring]\\n\",\n",
    "]\n",
    "\n",
    "responses = test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=100)\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Prompt {i + 1} - Response:\\n {response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:39:53.064457Z",
     "end_time": "2025-01-07T12:43:36.272703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.self_attn.rotary_emb\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.act_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.self_attn.rotary_emb\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.act_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.self_attn.rotary_emb\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.act_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.self_attn.rotary_emb\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.act_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.self_attn.rotary_emb\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.act_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.self_attn.rotary_emb\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.act_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.self_attn.rotary_emb\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.act_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.self_attn.rotary_emb\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.act_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.self_attn.rotary_emb\n",
      "model.layers.30.mlp\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.mlp.act_fn\n",
      "model.layers.30.input_layernorm\n",
      "model.layers.30.post_attention_layernorm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.self_attn.rotary_emb\n",
      "model.layers.31.mlp\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.mlp.act_fn\n",
      "model.layers.31.input_layernorm\n",
      "model.layers.31.post_attention_layernorm\n",
      "model.norm\n",
      "model.rotary_emb\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "# Show layers\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-07T12:43:55.869019Z",
     "end_time": "2025-01-07T12:43:55.949433Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ../Models/meta-llama/CodeLlama-7b-Python-hf_rune327e67d50b1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: float != c10::BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 154\u001B[0m\n\u001B[1;32m    151\u001B[0m         module \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;66;03m# Log model metrics to MLflow\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mlog_history:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2165\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2169\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:2524\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2517\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2518\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2520\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2521\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2522\u001B[0m )\n\u001B[1;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2530\u001B[0m ):\n\u001B[1;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:3654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3660\u001B[0m ):\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/trainer.py:3708\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3706\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3707\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3708\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3709\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3710\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/peft_model.py:1719\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1718\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1724\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1725\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1730\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1732\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1163\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m   1160\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1163\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1167\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1168\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1169\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1170\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1171\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1172\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1173\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1175\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1178\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:901\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    898\u001B[0m     all_hidden_states \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (hidden_states,)\n\u001B[1;32m    900\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_checkpointing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m--> 901\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gradient_checkpointing_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoder_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    903\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    905\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    906\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    907\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    908\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    909\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    910\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    911\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    913\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m    914\u001B[0m         hidden_states,\n\u001B[1;32m    915\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mcausal_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    922\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs,\n\u001B[1;32m    923\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    487\u001B[0m     dynamo_config_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 489\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    491\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:482\u001B[0m, in \u001B[0;36mcheckpoint\u001B[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001B[0m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m context_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m noop_context_fn \u001B[38;5;129;01mor\u001B[39;00m debug \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    478\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    479\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing `context_fn` or `debug` is only supported when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    480\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_reentrant=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    481\u001B[0m         )\n\u001B[0;32m--> 482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mCheckpointFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    484\u001B[0m     gen \u001B[38;5;241m=\u001B[39m _checkpoint_without_reentrant_generator(\n\u001B[1;32m    485\u001B[0m         function, preserve, context_fn, determinism_check, debug, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    486\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/autograd/function.py:553\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    552\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    561\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:261\u001B[0m, in \u001B[0;36mCheckpointFunction.forward\u001B[0;34m(ctx, run_function, preserve_rng_state, *args)\u001B[0m\n\u001B[1;32m    258\u001B[0m ctx\u001B[38;5;241m.\u001B[39msave_for_backward(\u001B[38;5;241m*\u001B[39mtensor_inputs)\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 261\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mrun_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    637\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    639\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 640\u001B[0m hidden_states, self_attn_weights, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    653\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:522\u001B[0m, in \u001B[0;36mLlamaSdpaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(\n\u001B[1;32m    510\u001B[0m         hidden_states\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[1;32m    511\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    517\u001B[0m         position_embeddings\u001B[38;5;241m=\u001B[39mposition_embeddings,\n\u001B[1;32m    518\u001B[0m     )\n\u001B[1;32m    520\u001B[0m bsz, q_len, _ \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m--> 522\u001B[0m query_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m key_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj(hidden_states)\n\u001B[1;32m    524\u001B[0m value_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj(hidden_states)\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:609\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[1;32m    607\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_layer(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 609\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    610\u001B[0m     torch_result_dtype \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    611\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m active_adapter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapters:\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/CryptoCounter/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected m1 and m2 to have the same dtype, but got: float != c10::BFloat16"
     ]
    }
   ],
   "source": [
    "# Fine-tuning configuration\n",
    "model_name = model_to_finetune\n",
    "\n",
    "# Generate a random run ID\n",
    "current_time = str(time.time()).encode('utf-8')\n",
    "hash_object = hashlib.sha256(current_time)\n",
    "hex_digest = hash_object.hexdigest()\n",
    "random_string = hex_digest[:12]\n",
    "run_id = random_string\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.3\n",
    "\n",
    "# Training parameter\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 1.5e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"adamw_torch\"\n",
    "lr_scheduler_type = \"constant_with_warmup\"\n",
    "max_steps = 20000\n",
    "warmup_ratio = 0.01\n",
    "group_by_length = True\n",
    "save_steps = 100\n",
    "logging_steps = 1\n",
    "eval_steps = 2\n",
    "\n",
    "# Output directory\n",
    "run_name = f\"{model_name}_run{run_id}\"\n",
    "output_dir = os.path.join(base_output_dir, run_name)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = os.path.join(output_dir, \"end_of_training\")\n",
    "\n",
    "# Target modules to adapt key components to the model type (Falcon / CodeLlama):\n",
    "# - Attention Projections: Query, Key, Value, and Output\n",
    "# - Feed-Forward Network: Input (Expansion) and Output (Reduction)\n",
    "# - Embedding Matrix: Maps tokens to dense vectors\n",
    "if \"CodeLlama\" in model_name:\n",
    "    target_modules = [\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "        \"embed_tokens\",\n",
    "    ]\n",
    "elif \"falcon\" in model_name:\n",
    "    target_modules = [\n",
    "        \"self_attention.query_key_value\",\n",
    "        \"self_attention.dense\",\n",
    "        \"mlp.dense_h_to_4h\",\n",
    "        \"mlp.dense_4h_to_h\",\n",
    "        \"word_embeddings\",\n",
    "    ]\n",
    "else:\n",
    "    target_modules = None\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"all\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "\n",
    "# Monitoring\n",
    "projectname='DocstringGenerator'\n",
    "\n",
    "# Initialize MLflow\n",
    "if on_colab:\n",
    "    # Starts MLflow UI in the background\n",
    "    get_ipython().system_raw(\"mlflow ui --backend-store-uri file:/content/mlruns --port 5000 &\")\n",
    "    # Forward port 5000 via ngrok\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(\"MLflow Tracking UI:\", public_url.public_url)\n",
    "    mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
    "else:\n",
    "    # run in terminal:\n",
    "    # mlflow server --host 127.0.0.1 --port 8080\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(projectname)\n",
    "mlflow.start_run(run_name=f\"run_{run_id}\")\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.login(key=wandb_api_token)\n",
    "wandb.init(\n",
    "    project=projectname,\n",
    "    name=f\"run_{run_id}\",\n",
    "    config={\n",
    "    \"lora_r\":lora_r,\n",
    "    \"lora_dropout\":lora_dropout,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"num_train_epochs\": num_train_epochs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    report_to=[\"wandb\", \"mlflow\"],\n",
    "    run_name=run_id,\n",
    "    logging_dir=os.path.join(base_output_dir, \"Results/runs/\", run_name),\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(),\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # max_seq_length=None,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    # packing=False,\n",
    ")\n",
    "\n",
    "# Pre-process the model of layer norm for stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Log model metrics to MLflow\n",
    "if trainer.state.log_history:\n",
    "    metrics = trainer.state.log_history[-1]\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "# Save the trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "mlflow.log_artifacts(output_dir)\n",
    "\n",
    "# End MLflow and W&B session\n",
    "mlflow.end_run()\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
